{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import tensorboardX\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AudioFrameDataset(Dataset):\n",
    "    '''Train only'''\n",
    "\n",
    "    def __init__(self, path_to_data, path_to_split, phase, transform=None, seed=14): \n",
    "        self.path_to_data = path_to_data\n",
    "        voice_set_labels = pd.read_table(path_to_split, sep=' ', names=['path', 'phase'])\n",
    "        voice_set_labels.replace({'_000': '/0', '.wav$': ''}, inplace=True, regex=True)\n",
    "        \n",
    "        # if-else conditions are bullshit there's no need for them because \n",
    "        # we need to use another file with predefined pairs for evaluation purposes\n",
    "        if phase == 1:\n",
    "            mask = (voice_set_labels.phase == 1) | (voice_set_labels.phase == 3)\n",
    "            dataset = voice_set_labels[mask].reset_index(drop=True)\n",
    "\n",
    "        else:\n",
    "            mask = voice_set_labels['phase'] == phase\n",
    "            dataset = voice_set_labels[mask].reset_index(drop=True)\n",
    "            \n",
    "        self.dataset = dataset['path']\n",
    "        self.transform = transform\n",
    "        self.seed = np.random.RandomState(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        ### VISUAL INPUT\n",
    "        video_path = os.path.join(self.path_to_data, 'video', self.dataset[idx] + '.txt')\n",
    "        frames = pd.read_table(video_path, skiprows=6, usecols=['FRAME '])\n",
    "        out_f.write('After pd.read_table: +{:.2f}\\n'.format(time.time() - t0))\n",
    "        earliest = frames['FRAME '].iloc[0]\n",
    "        latest = frames['FRAME '].iloc[-1]\n",
    "        frame_list = np.arange(earliest, latest+1)\n",
    "        mask = np.where(frame_list % 25 == 0)\n",
    "        out_f.write('After np.where: +{:.2f}\\n'.format(time.time() - t0))\n",
    "        # only 20 per each face-track (see the asterics on the project page)\n",
    "        # frames_sec = frame_list[mask]\n",
    "        frames_sec = frame_list[mask][:20]\n",
    "        selected_frame = self.seed.choice(frames_sec)\n",
    "        selected_frame_filename = '{0:07d}.jpg'.format(selected_frame)\n",
    "        selected_frame_path = os.path.join(self.path_to_data, 'video', \n",
    "                                           self.dataset[idx][:-5] + selected_frame_filename)\n",
    "#         out_f.write(selected_frame_path + '\\n')\n",
    "        out_f.write('Before reading image with opencv: +{:.2f}\\n'.format(time.time() - t0))\n",
    "        frame = cv2.cvtColor(cv2.imread(selected_frame_path), cv2.COLOR_BGR2RGB)\n",
    "        out_f.write('After reading image with opencv: +{:.2f}\\n'.format(time.time() - t0))\n",
    "        frame = cv2.resize(frame, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "        out_f.write('After resizing the image: +{:.2f}\\n'.format(time.time() - t0))\n",
    "        out_f.write('Total after image preparation: +{:.2f}\\n'.format(time.time() - t0))\n",
    "        \n",
    "        ### AUDIO INPUT\n",
    "        segment_len = 3\n",
    "        # not 1024 as reported in the referenced paper [35] \n",
    "        # because it gives 513xN but not 512xN\n",
    "        FFT_len = 1022\n",
    "        window = 'hamming'\n",
    "        audio_path = os.path.join(self.path_to_data, 'audio', self.dataset[idx] + '.wav')\n",
    "        out_f.write('After path is chosen: +{:.2f}\\n'.format(time.time() - t0))\n",
    "#         out_f.write(audio_path + '\\n')\n",
    "        sample_rate, samples = wavfile.read(audio_path)\n",
    "        out_f.write('After wavefile has been read: +{:.2f}\\n'.format(time.time() - t0))\n",
    "        window_width = int(sample_rate * 0.025)\n",
    "        overlap = int(sample_rate * (0.025 - 0.010))\n",
    "        start = len(samples) - segment_len * sample_rate\n",
    "        end = start + segment_len * sample_rate\n",
    "        audio_segment = samples[start:end]\n",
    "        # Note, it produces 512x298 and I don't know why there is some subtle\n",
    "        # differences. However, since the model averages second axis it doesn't\n",
    "        # matter from the computational POV.\n",
    "        out_f.write('After cropping the sample: +{:.2f}\\n'.format(time.time() - t0))\n",
    "        _, _, spectrogram = signal.spectrogram(audio_segment, sample_rate, \n",
    "                                               window=window, nfft=FFT_len, \n",
    "                                               nperseg=window_width, noverlap=overlap, \n",
    "                                               mode='magnitude')\n",
    "        out_f.write('After spectogram calculation: +{:.2f}\\n'.format(time.time() - t0))\n",
    "#         log_spectogram = np.log(spectrogram)\n",
    "        log_spectogram = spectrogram.copy()\n",
    "        out_f.write('After spectrogram.copy(): +{:.2f}\\n'.format(time.time() - t0))\n",
    "        assert sample_rate == 16000\n",
    "        assert len(samples) >= sample_rate * segment_len\n",
    "        \n",
    "#         out_f.write(np.array_str(frame) + '\\n')\n",
    "#         out_f.write(np.array_str(log_spectogram) + '\\n')\n",
    "        \n",
    "        if self.transform:\n",
    "            frame = frame.astype(np.float32)\n",
    "            log_spectogram = log_spectogram.astype(np.float32)\n",
    "            frame, log_spectogram = self.transform((frame, log_spectogram))\n",
    "        out_f.write('After transforms(): +{:.2f}\\n'.format(time.time() - t0))\n",
    "        out_f.write('Total time elapsed for one index: +{:.2f}\\n\\n'.format(time.time() - t0))\n",
    "        \n",
    "        return frame, log_spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"Normalizes both face (mean) and voice spectogram (mean-varience)\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        frame, log_spectogram = sample\n",
    "\n",
    "        ## FACE (H, W, C)\n",
    "        # mean normalization for every image (not batch)\n",
    "        mu = frame.mean(axis=(0, 1))\n",
    "        frame = frame - mu\n",
    "        \n",
    "        ## VOICE (Freq, Time)\n",
    "        # mean-variance normalization for every spectogram (not batch)\n",
    "        mu = log_spectogram.mean(axis=1).reshape(512, 1)\n",
    "        sigma = log_spectogram.std(axis=1).reshape(512, 1)\n",
    "        log_spectogram = (log_spectogram - mu) / sigma\n",
    "\n",
    "        return frame, log_spectogram\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    '''Horizontally flip the given Image ndarray randomly with a given probability.'''\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        frame, log_spectogram = sample\n",
    "\n",
    "        if random.random() < self.p:\n",
    "            return cv2.flip(frame, 1), log_spectogram\n",
    "        \n",
    "        return frame, log_spectogram\n",
    "\n",
    "class ColorJittering(object):\n",
    "    '''Given Image ndarray performs brightness and \n",
    "    saturation jittering. It is not mentioned in the paper but I guess \n",
    "    the authors used MatConvNet but do not mention any specific augmentation\n",
    "    parameters. So, I made my wind guess regarding the parameters and implemented\n",
    "    augmentation in the following fashion as in there:\n",
    "    http://www.vlfeat.org/matconvnet/mfiles/vl_imreadjpeg/\n",
    "    and the Section 3.5 of the manual\n",
    "    http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf'''\n",
    "    \n",
    "    def __init__(self, brightness=[255/25.5, 255/25.5, 255/25.5], saturation=0.5):\n",
    "        # brightness\n",
    "        self.B = np.array(brightness, dtype=np.float32)\n",
    "        # saturation\n",
    "        self.S = saturation\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        frame, log_spectogram = sample\n",
    "        \n",
    "        # brightness\n",
    "        # gives an error w/o float32 -- normal() returns float64\n",
    "        w = np.float32(np.random.normal(size=3))\n",
    "        b = self.B * w\n",
    "        frame = np.clip(frame + b, 0, 255)\n",
    "        \n",
    "        # saturation\n",
    "        sigma = np.random.uniform(1-self.S, 1+self.S)\n",
    "        frame = sigma * frame + (1-sigma) / 3 * frame.sum(axis=2, keepdims=True)\n",
    "        frame = np.clip(frame, 0, 255)\n",
    "        \n",
    "        return frame, log_spectogram\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        frame, log_spectogram = sample\n",
    "        F, T = log_spectogram.shape\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        frame = frame.transpose((2, 0, 1))\n",
    "        \n",
    "        # now log_specs are of size (Freq, Time) 2D but has to be 3D\n",
    "        log_spectogram = log_spectogram.reshape(1, F, T)\n",
    "\n",
    "        return torch.from_numpy(frame), torch.from_numpy(log_spectogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "img = cv2.imread('/home/vladimir/storage8tb/data/voxceleb1/video/Bellamy_Young/vZY1f9oeIbo/0001350.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img, (224, 224))\n",
    "img = cv2.flip(img, 1)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TRY TO ADD DROPOUT\n",
    "\n",
    "class FaceSubnet(nn.Module):\n",
    "\n",
    "    def __init__(self, seed=13):\n",
    "        super(FaceSubnet, self).__init__()\n",
    "#         torch.manual_seed(seed)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=96)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=256)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        self.fc6 = nn.Linear(in_features=256 * 7 * 7, out_features=4096)\n",
    "        self.fc7 = nn.Linear(in_features=4096, out_features=1024)\n",
    "        self.fc8 = nn.Linear(in_features=1024, out_features=256)\n",
    "        \n",
    "        self.mpool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        \n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.mpool(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.mpool(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.mpool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc6(x))\n",
    "        x = self.relu(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "        \n",
    "        return F.normalize(x)\n",
    "\n",
    "## TRY TO REMOVE DROPOUT\n",
    "\n",
    "class VoiceSubnet(nn.Module):\n",
    "\n",
    "    def __init__(self, seed=13):\n",
    "        super(VoiceSubnet, self).__init__()\n",
    "#         torch.manual_seed(seed)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=7, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=96)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=256)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        self.bn6 = nn.BatchNorm2d(num_features=4096)\n",
    "        \n",
    "        self.fc7 = nn.Linear(in_features=4096, out_features=1024)\n",
    "        self.fc8 = nn.Linear(in_features=1024, out_features=256)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.mpool5 = nn.MaxPool2d(kernel_size=(5, 3), stride=(3, 2))\n",
    "        \n",
    "        # Conv2d with weights of size (H, 1) is identical to FC with H weights\n",
    "        self.fc6 = nn.Conv2d(in_channels=256, out_channels=4096, kernel_size=(9, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.mpool1(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.mpool2(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.mpool5(x)\n",
    "\n",
    "        B, C, H, W = x.size()\n",
    "        self.apool6 = nn.AvgPool2d(kernel_size=(1, W))\n",
    "\n",
    "        x = self.relu(self.bn6(self.fc6(x)))\n",
    "        x = self.apool6(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "        \n",
    "        return F.normalize(x)\n",
    "    \n",
    "class CurriculumMining(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CurriculumMining, self).__init__()\n",
    "        \n",
    "    def forward(self, positive_pairs, tau):\n",
    "        faces, voices = positive_pairs\n",
    "        B, D = faces.size()\n",
    "        # calc dist \n",
    "        # (X - Y) ^ 2 = X^2 + Y^2 - 2XY\n",
    "        x = (faces**2).sum(dim=1).view(-1, 1) + (voices**2).sum(dim=1) - 2*faces.matmul(voices.t())\n",
    "        dists = x.sqrt()\n",
    "        \n",
    "        sorted_dist, sorted_idx = torch.sort(dists, dim=1, descending=True)\n",
    "        Dnj = sorted_dist - dists.diag().view(-1, 1)\n",
    "        idx_threshold = round(tau * (B-1))\n",
    "        \n",
    "        # tricky part\n",
    "        mask = torch.ones_like(sorted_dist)\n",
    "        mask[:, idx_threshold+1:] = 0\n",
    "        mask[Dnj <= 0] = 0\n",
    "        idx_of_sorted_idx = ((mask).sum(dim=1) - 1).abs().long()\n",
    "        neg_samples_idx = torch.gather(sorted_idx, dim=1, index=idx_of_sorted_idx.view(B, 1))\n",
    "        neg_samples_idx = neg_samples_idx.view(B)\n",
    "        negative_voices = voices[neg_samples_idx]\n",
    "        \n",
    "        return (faces, negative_voices)\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, positive_pairs, negative_pairs, margin):\n",
    "        ## POSITIVE PART\n",
    "        faces, voices = positive_pairs\n",
    "#         dists_pos = ((faces - voices) ** 2).sum(dim=1).sqrt()\n",
    "#         pos_part = dists_pos ** 2\n",
    "        pos_part = ((faces - voices) ** 2).sum(dim=1)\n",
    "    \n",
    "        ## NEGATIVE PART\n",
    "        faces, voices = negative_pairs\n",
    "        dists_neg = ((faces - voices) ** 2).sum(dim=1).sqrt()\n",
    "        neg_part = (margin - dists_neg).clamp(0) ** 2\n",
    "        \n",
    "        loss4pair = torch.cat([pos_part, neg_part])\n",
    "        \n",
    "        ## CALCULATE LOSS\n",
    "        B, D = faces.size()\n",
    "        batch_loss = loss4pair.sum() / (2 * B)\n",
    "    \n",
    "        return batch_loss\n",
    "\n",
    "class LearnablePinsNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LearnablePinsNet, self).__init__()\n",
    "        self.face_subnet = FaceSubnet()\n",
    "        self.voice_subnet = VoiceSubnet()\n",
    "        self.curr_mining = CurriculumMining()\n",
    "        \n",
    "    def forward(self, frames, log_specs, tau=None):\n",
    "        emb_f = self.face_subnet(frames)\n",
    "        emb_v = self.voice_subnet(log_specs)\n",
    "        \n",
    "        if self.training:\n",
    "            positive_pairs = (emb_f, emb_v)\n",
    "            negative_pairs = self.curr_mining(positive_pairs, tau)\n",
    "\n",
    "            return positive_pairs, negative_pairs\n",
    "        \n",
    "        else:\n",
    "            return (emb_f, emb_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TauScheduler(object):\n",
    "    '''\n",
    "    ## TODO: new doc str\n",
    "    [trash]:\n",
    "    \"found that it was effective to increase \\tau by 10 percent \n",
    "    every two epochs, starting from 30% up until 80%, and keeping \n",
    "    it constant thereafter\"\n",
    "    --- So, it is increasing up to 10th epoch where becomes 0.8\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, lowest, highest):\n",
    "        self.current = int(lowest * 100)\n",
    "        self.highest = int(highest * 100)\n",
    "        self.epoch_num = 0\n",
    "\n",
    "    def step(self):\n",
    "            \n",
    "        if self.epoch_num % 2 == 0 and self.epoch_num > 0:\n",
    "#                 self.current += 10\n",
    "            self.current = int(self.current + self.current * 0.1)\n",
    "        \n",
    "        if self.current > self.highest:\n",
    "            self.current = 80\n",
    "    \n",
    "        self.epoch_num += 1\n",
    "        \n",
    "    def get_tau(self):\n",
    "        return self.current / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau_scheduler = TauScheduler(0.3, 0.8)\n",
    "# for i in range(50):\n",
    "#     tau_scheduler.step()\n",
    "#     print(i, tau_scheduler.get_tau())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = '/home/vladimir/storage8tb/logs/LearnablePINs7/'\n",
    "TXT_LOG_PATH = os.path.join(LOG_PATH, 'debug_log.txt')\n",
    "DATA_PATH = '/home/vladimir/storage8tb/data/voxceleb1/'\n",
    "USE_FILTERED = True\n",
    "\n",
    "if USE_FILTERED:\n",
    "    SPLIT_PATH = os.path.join(DATA_PATH, 'Splits/filtered_voice_set_labels.txt')\n",
    "\n",
    "else:\n",
    "    SPLIT_PATH = os.path.join(DATA_PATH, 'Splits/voice_set_labels.txt')\n",
    "\n",
    "FACE_SUBNET_SNAPSHOT_PATH = os.path.join(LOG_PATH, 'face_subnet_snapshot.txt')\n",
    "VOICE_SUBNET_SNAPSHOT_PATH = os.path.join(LOG_PATH, 'voice_subnet_snapshot.txt')\n",
    "PHASE = 1\n",
    "DEVICES = [0, 1, 2]\n",
    "B = 85 * len(DEVICES)\n",
    "# https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/5\n",
    "NUM_WORKERS = 4 * len(DEVICES)\n",
    "MARGIN = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_f = open(TXT_LOG_PATH, 'w')\n",
    "TBoard = tensorboardX.SummaryWriter(log_dir=LOG_PATH)\n",
    "\n",
    "try:\n",
    "    transform = Compose([\n",
    "        Normalize(),\n",
    "        RandomHorizontalFlip(),\n",
    "        ColorJittering(),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train = AudioFrameDataset(DATA_PATH, SPLIT_PATH, phase=PHASE, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(train, batch_size=B, num_workers=NUM_WORKERS)\n",
    "\n",
    "    net = LearnablePinsNet()\n",
    "\n",
    "    criterion = ContrastiveLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9, weight_decay=5e-4)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=10 ** (-3/25))\n",
    "    tau_scheduler = TauScheduler(lowest=0.3, highest=0.8)\n",
    "\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.set_device(DEVICES[0])\n",
    "    net.to(device);\n",
    "    net = nn.DataParallel(net, DEVICES)\n",
    "\n",
    "    for epoch_num in range(50):\n",
    "        net.train()\n",
    "        lr_scheduler.step()\n",
    "        tau_scheduler.step()\n",
    "\n",
    "        for iter_num, (frames, log_specs) in tqdm(enumerate(trainloader)):\n",
    "            step_num = epoch_num * len(trainloader) + iter_num\n",
    "            out_f.write('{}-{}\\n'.format(epoch_num, iter_num))\n",
    "            t0 = time.time()\n",
    "\n",
    "            # transfer inputs to a device\n",
    "            frames, log_specs = frames.cuda(async=True), log_specs.cuda(async=True)\n",
    "            frames, log_specs = torch.autograd.Variable(frames), torch.autograd.Variable(log_specs)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            positive_pairs, negative_pairs = net(frames, log_specs, tau=tau_scheduler.get_tau())\n",
    "#             TBoard.add_scalar('Timing/InferenceTime', time.time() - t0, step_num)\n",
    "            loss = criterion(positive_pairs, negative_pairs, margin=MARGIN)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            TBoard.add_scalar('Train/Loss', loss.item(), step_num)\n",
    "            TBoard.add_scalar('Train/lr', lr_scheduler.get_lr()[0], step_num)\n",
    "            TBoard.add_scalar('Train/tau', tau_scheduler.get_tau(), step_num)\n",
    "            TBoard.add_scalar('emb_mean/face', positive_pairs[0].mean(), step_num)\n",
    "            TBoard.add_scalar('emb_mean/voice', positive_pairs[1].mean(), step_num)\n",
    "\n",
    "            if hasattr(net, 'module'):\n",
    "                TBoard.add_scalar('weights/voice_conv1', net.module.voice_subnet.conv1.weight.mean(), step_num)\n",
    "                TBoard.add_scalar('weights/face_conv1', net.module.face_subnet.conv1.weight.mean(), step_num)\n",
    "                TBoard.add_scalar('weights/voice_conv5', net.module.voice_subnet.conv5.weight.mean(), step_num)\n",
    "                TBoard.add_scalar('weights/face_conv5', net.module.face_subnet.conv5.weight.mean(), step_num)\n",
    "\n",
    "            else:\n",
    "                TBoard.add_scalar('weights/voice_conv1', net.voice_subnet.conv1.weight.mean(), step_num)\n",
    "                TBoard.add_scalar('weights/face_conv1', net.face_subnet.conv1.weight.mean(), step_num)\n",
    "                TBoard.add_scalar('weights/voice_conv5', net.voice_subnet.conv5.weight.mean(), step_num)\n",
    "                TBoard.add_scalar('weights/face_conv5', net.face_subnet.conv5.weight.mean(), step_num)\n",
    "\n",
    "#             TBoard.add_scalar('raw_input/face_mean', frames.mean(), step_num)\n",
    "#             TBoard.add_scalar('raw_input/voice_mean', log_specs.mean(), step_num)\n",
    "#             TBoard.add_scalar('Timing/IterTime', time.time() - t0, step_num)\n",
    "            \n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    out_f.close()\n",
    "    TBoard.close()\n",
    "    print('TBoard and log file has been closed!')\n",
    "    \n",
    "out_f.close()\n",
    "TBoard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.module.face_subnet.state_dict(), FACE_SUBNET_SNAPSHOT_PATH)\n",
    "torch.save(net.module.voice_subnet.state_dict(), VOICE_SUBNET_SNAPSHOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
